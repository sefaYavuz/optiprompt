services:
  litellm:
    image: ghcr.io/berriai/litellm:main-latest
    volumes:
     - ./litellm_config.yaml:/app/config.yaml
    command:
     - "--config=/app/config.yaml"
    ports:
      - "4000:4000" # Map the container port to the host, change the host port if necessary
    environment:
      DATABASE_URL: "postgresql://llmproxy:dbpassword9090@db:5432/litellm"
      STORE_MODEL_IN_DB: "True" # allows adding models to proxy via UI
      REDIS_URL: "redis://redis:6379"  # <--- Added Redis URL
    env_file:
      - .env # Load local .env file
    depends_on:
      - db  # Ensure db starts first
      - redis  # <--- Added Redis as a dependency
    healthcheck:
      test: [ "CMD-SHELL", "wget --no-verbose --tries=1 http://localhost:4000/health/liveliness || exit 1" ]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s

  db:
    image: postgres:16
    restart: always
    container_name: litellm_db
    environment:
      POSTGRES_DB: litellm
      POSTGRES_USER: llmproxy
      POSTGRES_PASSWORD: dbpassword9090
    ports:
      - "5432:5432"
    volumes:
      - postgres_data:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -d litellm -U llmproxy"]
      interval: 1s
      timeout: 5s
      retries: 10

  redis:
    image: redis:7
    restart: always
    ports:
      - "6379:6379"

volumes:
  postgres_data:
    name: litellm_postgres_data

